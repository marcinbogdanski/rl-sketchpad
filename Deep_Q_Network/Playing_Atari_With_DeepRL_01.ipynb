{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "$$ \\huge{\\underline{\\textbf{ Playing Atari Games with Deep RL }}} $$\n",
    "\n",
    "$$ \\large{\\textbf{MountainCar + DQN + Memory Reply}} $$\n",
    "\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, frames, gamma, eps_decay_steps, eps_target, action_repeat,\n",
    "               batch_size, model, mem, callback=None, trace=None):\n",
    "    \"\"\"Episodic Semi-Gradient Sarsa\n",
    "    \n",
    "    Params:\n",
    "        env - environment\n",
    "        ep - number of episodes to run\n",
    "        gamma - discount factor [0..1]\n",
    "        eps - epsilon-greedy param\n",
    "        model      - function approximator, already initialised, with methods:\n",
    "                     eval(state, action) -> float\n",
    "                     train(state, target) -> None\n",
    "    \"\"\"\n",
    "    def policy(st, model, eps):\n",
    "        if np.random.rand() > eps:\n",
    "            q_values = model.eval(np.array([st]))\n",
    "            return np.argmax(q_values)\n",
    "        else:\n",
    "            return env.action_space.sample()\n",
    "    \n",
    "    if eps_decay_steps is not None:\n",
    "        eps_delta = (1-eps_target) / eps_decay_steps\n",
    "        eps = 1\n",
    "    else:\n",
    "        eps = eps_target\n",
    "        \n",
    "    # Fill memory buffer using random policy\n",
    "    while len(mem) < mem.max_len:\n",
    "        S = env.reset();\n",
    "        for t_ in itertools.count():\n",
    "            if t_ % action_repeat == 0:\n",
    "                A = env.action_space.sample()    # random policy\n",
    "            S_, R, done, _ = env.step(A)\n",
    "            mem.append(S, A, R, S_, done)\n",
    "            if done:\n",
    "                break\n",
    "            S = S_\n",
    "\n",
    "    tts_ = 0                                 # total time step\n",
    "    for e_ in itertools.count():             # count from 0 to infinity\n",
    "        \n",
    "        S = env.reset()\n",
    "        \n",
    "        for t_ in itertools.count():         # count from 0 to infinity\n",
    "            \n",
    "            if t_ % action_repeat == 0:\n",
    "                A = policy(S, model, eps)\n",
    "            \n",
    "            S_, R, done, _ = env.step(A)\n",
    "            \n",
    "            mem.append(S, A, R, S_, done)\n",
    "            \n",
    "            if callback is not None:\n",
    "                callback(e_, t_, S, A, R, done, eps, model, mem, trace)\n",
    "            \n",
    "            states, actions, rewards, n_states, dones, _ = mem.get_batch(batch_size)\n",
    "            targets = model.eval(n_states)\n",
    "            targets = rewards + gamma * np.max(targets, axis=-1)\n",
    "            targets[dones] = rewards[dones]                # return of next-to-terminal state is just R\n",
    "            model.train(states, actions, targets)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if tts_ >= frames:\n",
    "                return\n",
    "                \n",
    "            S = S_\n",
    "            \n",
    "            if eps > eps_target:\n",
    "                eps = max(eps - eps_delta, eps_target)\n",
    "                \n",
    "            tts_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enjoy_env(env, frames, eps, action_repeat, model, callback=None, trace=None, render=True, sleep=0):\n",
    "\n",
    "    def policy(st, model, eps):\n",
    "        if np.random.rand() > eps:\n",
    "            q_values = model.eval(np.array([st]))\n",
    "            return np.argmax(q_values)\n",
    "        else:\n",
    "            return env.action_space.sample()\n",
    "    \n",
    "    tts_ = 0                                 # total time step\n",
    "    for e_ in itertools.count():             # count from 0 to infinity\n",
    "        \n",
    "        S = env.reset()\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(sleep)\n",
    "        \n",
    "        for t_ in itertools.count():         # count from 0 to infinity\n",
    "            \n",
    "            if t_ % action_repeat == 0:\n",
    "                A = policy(S, model, eps)\n",
    "            \n",
    "            S_, R, done, _ = env.step(A)\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(sleep)\n",
    "            \n",
    "            if callback is not None:\n",
    "                callback(e_, t_, S, A, R, done, eps, model, mem, trace)\n",
    "    \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if tts_ >= frames:\n",
    "                return\n",
    "                \n",
    "            S = S_\n",
    "                \n",
    "            tts_ += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports (source file: [tiles3.py](tiles3.py), [helpers_1001.py](helpers_1001.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tables\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "with tf.Session(config=config) as sess:\n",
    "    devs = sess.list_devices()\n",
    "    print('\\n'.join([x.name for x in devs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "import importlib\n",
    "importlib.reload(helpers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need callback to capture q-value array for whole state-action space at specified episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trace():\n",
    "    def __init__(self, tf_summary_writer, eval_every, render=False, test_states=None, test_labels=None, state_labels=None):\n",
    "        if test_states is not None:\n",
    "            assert test_states.ndim == 2\n",
    "            assert len(test_states) == len(test_labels)\n",
    "            \n",
    "        self.tf_summary_writer = tf_summary_writer\n",
    "        \n",
    "        \n",
    "        self.eval_every = eval_every\n",
    "        self.render = render\n",
    "        self.test_states = test_states\n",
    "        self.test_labels = test_labels\n",
    "        self.state_labels = state_labels\n",
    "        \n",
    "        self.total_tstep = 0\n",
    "        \n",
    "        self.q_values = collections.OrderedDict()\n",
    "        self.ep_end_idx = collections.OrderedDict()\n",
    "        self.ep_rewards = collections.defaultdict(float)\n",
    "        \n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []  # t+1\n",
    "        self.dones = []    # t+1\n",
    "        self.epsilons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(episode, tstep, st, act, rew_, done_, eps, model, memory, trace):\n",
    "    \"\"\"Called from gradient_MC after every episode.\n",
    "    \n",
    "    Params:\n",
    "        episode [int] - episode number\n",
    "        tstep [int]   - timestep within episode\n",
    "        model [obj]   - function approximator\n",
    "        trace [list]  - list to write results to\"\"\"\n",
    "    \n",
    "    if trace.total_tstep == 0 and memory is not None:\n",
    "        print('num exits in memory:', np.count_nonzero(memory._hist_done_1))\n",
    "        \n",
    "    if done_:\n",
    "        trace.ep_end_idx[episode] = trace.total_tstep\n",
    "    \n",
    "    trace.states.append(st)\n",
    "    trace.actions.append(act)\n",
    "    trace.rewards.append(rew_)\n",
    "    trace.dones.append(done_)\n",
    "    trace.epsilons.append(eps)\n",
    "    \n",
    "    trace.ep_rewards[episode] += rew_\n",
    "    \n",
    "    if done_ and trace.tf_summary_writer is not None:\n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag='episode_reward', simple_value=trace.ep_rewards[episode])\n",
    "        trace.tf_summary_writer.add_summary(summary, trace.total_tstep)\n",
    "    \n",
    "    if trace.render:\n",
    "        env.render()\n",
    "            \n",
    "    if trace.eval_every is not None:\n",
    "        if trace.total_tstep % trace.eval_every == 0:\n",
    "            print()\n",
    "            print('â– '*80)\n",
    "            print('episode:', episode, '\\t time step:', tstep,\n",
    "                  '\\t total time step:', trace.total_tstep, '\\t eps:', round(eps,3), \n",
    "                  '\\t wall time:', datetime.datetime.now())\n",
    "\n",
    "            \n",
    "        if trace.total_tstep % trace.eval_every == 0:\n",
    "            \n",
    "            if len(st) == 2:\n",
    "                # We are working with 2D environment,\n",
    "                # plot whole Q-Value functions across whole state space\n",
    "            \n",
    "                q_arr = helpers.eval_state_action_space(model, env, split=[128,128])\n",
    "                trace.q_values[trace.total_tstep] = q_arr\n",
    "\n",
    "                helpers.plot_2d_environment(env, trace.total_tstep, 1000, trace, memory,\n",
    "                                            axis_labels=['state[0]', 'state[1]'],\n",
    "                                            action_labels=['Act 0', 'Act 1', 'Act 2'],\n",
    "                                            action_colors=['red', 'blue', 'green'])\n",
    "                \n",
    "            else:\n",
    "                # Environment is not 2D, so we can't plot whole Q-Value function\n",
    "                # Instead we plot state on standard graph, which is still better than nothing\n",
    "                \n",
    "                if trace.test_states is not None:\n",
    "                    y_hat = model.eval(trace.test_states)\n",
    "                    trace.q_values[trace.total_tstep] = y_hat\n",
    "                \n",
    "                helpers.plot_generic_environment(env, trace.total_tstep, 1000, trace, memory)\n",
    "\n",
    "    trace.total_tstep += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_lunarlander_tf(frames):\n",
    "    \n",
    "    tf_model = TFNeuralNet(nb_in=8, nb_hid_1=64, nb_hid_2=64, nb_out=4, lr=0.00025)\n",
    "    model = TFFunctApprox(tf_model,\n",
    "                          np.array([-1., -1., -1., -1., -1., -1., -1., -1.]),  # state space low\n",
    "                          np.array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]),\n",
    "                          rew_mean=0,\n",
    "                          rew_std=1,\n",
    "                          nb_actions=env.action_space.n)\n",
    "    \n",
    "    mem = Memory(max_len=100000, state_shape=(8,), state_dtype=float)\n",
    "    \n",
    "    trace = Trace(eval_every=1000,\n",
    "                 test_states=np.array([[0, 1.4, 0, 0, 0, 0, 0, 0],     # init\n",
    "                                       [0, 0.7, 0, 0, 0, 0, 0, 0],     # half way, no tilt\n",
    "                                       [0, 0.0, 0, 0, 0, 0, 0, 0],]),  # landing pad\n",
    "                 test_labels=['start', 'half-way', 'landing-pad'],\n",
    "                 state_labels=['Pos.x', 'Pos.y', 'Vel.x', 'Vel.y', 'Angle', 'Ang. Vel', 'Left Leg', 'Right Leg'])\n",
    "    \n",
    "    if frames != 0:\n",
    "        with tables.open_file('outarray.h5', mode='w') as f_:\n",
    "            model._model.setup_logdb(f_)\n",
    "            q_learning(env, frames=frames, gamma=.99, eps_decay_steps=50000, eps_target=0.1, action_repeat=4,\n",
    "                       batch_size=4096, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    return trace, model, mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, don't train\n",
    "env = gym.make('LunarLander-v2')\n",
    "trace_rl, model, mem = experiment_lunarlander_tf(frames=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train from scratch\n",
    "env = gym.make('LunarLander-v2')\n",
    "trace_rl, model, mem = experiment_lunarlander_tf(frames=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "model._model.save('./tf_models/LunarLander-v2.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights\n",
    "model._model.load('./tf_models/LunarLander-v2.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enjoy agent\n",
    "try:\n",
    "    enjoy_env(env, frames=float('inf'), eps=0.0, action_repeat=4, model=model, render=True, sleep=0.0)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acrobat Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_acrobot_tf(frames):\n",
    "    \n",
    "    tf_model = TFNeuralNet(nb_in=6, nb_hid_1=64, nb_hid_2=64, nb_out=3, lr=0.00025)\n",
    "    model = TFFunctApprox(tf_model,\n",
    "                          env.observation_space.low,\n",
    "                          env.observation_space.high,\n",
    "                          rew_mean=-70,\n",
    "                          rew_std=10,\n",
    "                          nb_actions=env.action_space.n)\n",
    "    \n",
    "    mem = Memory(max_len=100000, state_shape=(6,), state_dtype=float)\n",
    "    \n",
    "    trace = Trace(eval_every=1000)\n",
    "    \n",
    "    with tables.open_file('outarray.h5', mode='w') as f_:\n",
    "        model._model.setup_logdb(f_)\n",
    "        q_learning(env, frames=frames, gamma=.99, eps_decay_steps=20000, eps_target=0.1, action_repeat=4,\n",
    "                   batch_size=4096, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    #test_car(env, ep=20, model=model, callback=callback, trace=trace)\n",
    "    print()\n",
    "    return trace, model, mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "trace_rl, model, mem = experiment_acrobot_tf(frames=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enjoy_env(env, frames=500, eps=0.1, action_repeat=1, model=model, render=True, sleep=0.03)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_cartpole_tf(frames):\n",
    "    \n",
    "    # Note: This environment has peculiar dynamics where positive reward,\n",
    "    # along with non-corrected maximisation bias in Q-Function (we don't use Dual-DQN)\n",
    "    # causes runaway Q-Value effect where Q-Values increase continously\n",
    "    # This is ok for now, because RELATIVE Q-Values are still good enough\n",
    "    # I'm pretty sure using target network along with Dual-DQN is a correct proper fix for this\n",
    "    \n",
    "    # CartPole original low/high values for observatios are not very reasonable, so we replace them:\n",
    "    # env.observation_space.low: [-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38]\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    session = tf.Session()\n",
    "    summary_writer = tf.summary.FileWriter(logdir='tf_log_2/cartpole/1')\n",
    "\n",
    "    neural_net = TFNeuralNet(tf_session=session, tf_summary_writer=summary_writer,\n",
    "                             nb_in=4, nb_hid_1=64, nb_hid_2=64, nb_out=2, lr=0.0001)\n",
    "    \n",
    "    model = TFFunctApprox(neural_net,\n",
    "                          st_low=np.array([-.2, -1.0, -0.15, -1.0]),\n",
    "                          st_high=np.array([.2, 1.0, 0.15, 1.0]),\n",
    "                          rew_mean=0,\n",
    "                          rew_std=1,\n",
    "                          nb_actions=env.action_space.n)\n",
    "    \n",
    "    mem = Memory(max_len=100000, state_shape=(4,), state_dtype=float)\n",
    "    \n",
    "    trace = Trace(tf_summary_writer=summary_writer,\n",
    "                 eval_every=1000,\n",
    "                 test_states=np.array([[0, 0, 0, 0]]),\n",
    "                 test_labels=['start'],\n",
    "                 state_labels=['Cart Pos', 'Cart Vel', 'Pole Ang', 'Pole Vel'])\n",
    "\n",
    "    summary_writer.add_graph(session.graph)\n",
    "    summary_writer.flush()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "    with tables.open_file('outarray.h5', mode='w') as f_:\n",
    "        model._model.setup_logdb(f_)\n",
    "        q_learning(env, frames=frames, gamma=.99, eps_decay_steps=20000, eps_target=0.1, action_repeat=1,\n",
    "                   batch_size=4096, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    return trace, model, mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "trace_rl, model, mem = experiment_cartpole_tf(frames=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enjoy_env(env, frames=500, eps=0.1, action_repeat=1, model=model, render=True, sleep=0.03)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MountainCar - TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_tf(frames):\n",
    "    \n",
    "    tf_model = TFNeuralNet(nb_in=2, nb_hid_1=64, nb_hid_2=64, nb_out=3, lr=0.00025)\n",
    "    model = TFFunctApprox(tf_model,\n",
    "                          env.observation_space.low,\n",
    "                          env.observation_space.high,\n",
    "                          rew_mean=-50,\n",
    "                          rew_std=15,\n",
    "                          nb_actions=env.action_space.n)\n",
    "    \n",
    "    mem = Memory(max_len=100000, state_shape=(2,), state_dtype=float)\n",
    "    \n",
    "    trace = Trace(eval_every=1000)\n",
    "    \n",
    "    with tables.open_file('outarray.h5', mode='w') as f_:\n",
    "        model._model.setup_logdb(f_)\n",
    "        q_learning(env, frames=frames, gamma=.99, eps_decay_steps=20000, eps_target=0.1, action_repeat=4,\n",
    "                   batch_size=4096, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    return trace, model, mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, don't train\n",
    "env = gym.make('MountainCar-v0').env\n",
    "trace_rl, model, mem = experiment_tf(frames=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train from scratch\n",
    "env = gym.make('MountainCar-v0').env\n",
    "trace_rl, model, mem = experiment_tf(frames=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "model._model.save('./tf_models/MountainCar.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights\n",
    "model._model.load('./tf_models/MountainCar.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enjoy agent\n",
    "try:\n",
    "    enjoy_env(env, frames=float('inf'), eps=0.0, action_repeat=4, model=model, render=True, sleep=0.0)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MountainCar - Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_keras(frames):\n",
    "    \n",
    "    keras_model = tf.keras.models.Sequential()\n",
    "    keras_model.add(tf.keras.layers.Dense(256, 'relu', input_dim=2))\n",
    "    keras_model.add(tf.keras.layers.Dense(256, 'relu'))\n",
    "    keras_model.add(tf.keras.layers.Dense(3, 'linear'))\n",
    "    keras_model.compile(loss='mse', optimizer=tf.keras.optimizers.RMSprop(lr=0.00025))\n",
    "    \n",
    "    model = function_approximators.KerasFunctApprox(\n",
    "        keras_model, env.observation_space.low, env.observation_space.high, env.action_space.n)\n",
    "    \n",
    "    mem = Memory(max_len=100000, state_shape=(2,), state_dtype=float)\n",
    "    \n",
    "    trace = Trace(eval_every=1000)\n",
    "    q_learning(env, frames=frames, gamma=.99, eps_decay_steps=50000, eps_target=0.1, action_repeat=4,\n",
    "               batch_size=4096, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    #test_car(env, ep=20, model=model, callback=callback, trace=trace)\n",
    "    print()\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0').env\n",
    "trace_rl = experiment_keras(frames=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MountainCar Tiles (batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_tiles_batch(frames):\n",
    "    \n",
    "    model = function_approximators.TileCodingFuncApprox(\n",
    "        env.observation_space.low, env.observation_space.high, env.action_space.n,\n",
    "        learn_rate=0.3, num_tilings=8, init_val=0)\n",
    "    \n",
    "    mem = Memory(max_len=1000, state_shape=(2,), state_dtype=float)\n",
    "    \n",
    "    trace = Trace(eval_every=1000)\n",
    "    \n",
    "    q_learning(env, frames=frames, gamma=1.0, eps_decay_steps=None, eps_target=0.0, action_repeat=4,\n",
    "               batch_size=64, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0').env\n",
    "trace_t_b_1 = experiment_tiles_batch(frames=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MountainCar Tiles (batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_tiles_single(frames):\n",
    "    \n",
    "    model = function_approximators.TileCodingFuncApprox(\n",
    "        env.observation_space.low, env.observation_space.high, env.action_space.n,\n",
    "        learn_rate=0.3, num_tilings=8, init_val=0)\n",
    "    \n",
    "    mem = Memory(max_len=1, state_shape=(2,), state_dtype=float)\n",
    "    \n",
    "    trace = Trace(eval_every=1000)\n",
    "    q_learning(env, frames=frames, gamma=1.0, eps_decay_steps=None, eps_target=0.0, action_repeat=4,\n",
    "               batch_size=1, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0').env\n",
    "trace_t_s_1 = experiment_tiles_single(frames=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendulum2D - TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments import Pendulum2DEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_pendulum_tf(frames):\n",
    "    \n",
    "    tf_model = TFNeuralNet(nb_in=2, nb_hid_1=64, nb_hid_2=64, nb_out=3)\n",
    "    model = TFFunctApprox(tf_model,\n",
    "                          env.observation_space.low,\n",
    "                          env.observation_space.high,\n",
    "                          rew_mean=-210,\n",
    "                          rew_std=50,\n",
    "                          nb_actions=env.action_space.n)\n",
    "    \n",
    "    mem = Memory(max_len=100000, state_shape=(2,), state_dtype=float)\n",
    "    \n",
    "    trace = Trace(eval_every=1000)\n",
    "    \n",
    "    with tables.open_file('outarray.h5', mode='w') as f_:\n",
    "        model._model.setup_logdb(f_)\n",
    "        q_learning(env, frames=frames, gamma=.99, eps_decay_steps=20000, eps_target=0.1, action_repeat=1,\n",
    "                   batch_size=4096, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    #test_car(env, ep=20, model=model, callback=callback, trace=trace)\n",
    "    print()\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = Pendulum2DEnv()\n",
    "trace_rl = experiment_pendulum_tf(frames=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendulum2D - Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments import Pendulum2DEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_pendulum_keras(frames):\n",
    "    \n",
    "    keras_model = tf.keras.models.Sequential()\n",
    "    keras_model.add(tf.keras.layers.Dense(256, 'relu', input_dim=2))\n",
    "    keras_model.add(tf.keras.layers.Dense(256, 'relu'))\n",
    "    keras_model.add(tf.keras.layers.Dense(3, 'linear'))\n",
    "    keras_model.compile(loss='mse', optimizer=tf.keras.optimizers.RMSprop(lr=0.00025))\n",
    "    \n",
    "    model = function_approximators.KerasFunctApprox(\n",
    "        keras_model, env.observation_space.low, env.observation_space.high, env.action_space.n)\n",
    "    \n",
    "    mem = Memory(max_len=100000, state_shape=(2,), state_dtype=float)\n",
    "    \n",
    "    trace = Trace(eval_every=1000)\n",
    "    q_learning(env, frames=frames, gamma=.99, eps_decay_steps=50000, eps_target=0.1, action_repeat=1,\n",
    "               batch_size=4096, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    #test_car(env, ep=20, model=model, callback=callback, trace=trace)\n",
    "    print()\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = Pendulum2DEnv()\n",
    "trace_rl = experiment_pendulum_keras(frames=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendulum2D - Tiles (batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments import Pendulum2DEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_pendulum_tiles_single(frames):\n",
    "    \n",
    "    model = function_approximators.TileCodingFuncApprox(\n",
    "        env.observation_space.low, env.observation_space.high, env.action_space.n,\n",
    "        learn_rate=0.3, num_tilings=32, init_val=0)\n",
    "    \n",
    "    mem = Memory(max_len=1, state_shape=(2,), state_dtype=float)\n",
    "    \n",
    "    trace = Trace(eval_every=25000)\n",
    "    q_learning(env, frames=frames, gamma=1.0, eps_decay_steps=None, eps_target=0.0, action_repeat=1,\n",
    "               batch_size=1, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    #test_car(env, ep=20, model=model, callback=callback, trace=trace)\n",
    "    print()\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = Pendulum2DEnv()\n",
    "experiment_pendulum_tiles_single(frames=100000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Approximators and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFNeuralNet():\n",
    "    def __init__(self, tf_session, tf_summary_writer, nb_in, nb_hid_1, nb_hid_2, nb_out, lr):\n",
    "        \n",
    "        self._sess = tf_session\n",
    "        self._summary_writer = tf_summary_writer\n",
    "        \n",
    "        self.nb_in = nb_in\n",
    "        self.nb_hid_1 = nb_hid_1\n",
    "        self.nb_hid_2 = nb_hid_2\n",
    "        self.nb_out = nb_out\n",
    "        \n",
    "        self._f = None\n",
    "        self._timestep = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('NeuralNet'):\n",
    "        \n",
    "            with tf.variable_scope('zz_Inputs'):\n",
    "                self._x = tf.placeholder(name='xx', shape=[None, nb_in], dtype=tf.float32)\n",
    "                self._y = tf.placeholder(name='yy', shape=[None, nb_out], dtype=tf.float32)\n",
    "\n",
    "                tf.summary.histogram('xx', self._x)\n",
    "                tf.summary.histogram('yy', self._y)\n",
    "                \n",
    "            with tf.variable_scope('Hidden_1'):\n",
    "                self._W_hid_1 = tf.get_variable('hid_1_W', shape=[nb_in, nb_hid_1], dtype=tf.float32)\n",
    "                self._b_hid_1 = tf.get_variable('hid_1_b', shape=[nb_hid_1], dtype=tf.float32,\n",
    "                                                initializer=tf.zeros_initializer())\n",
    "                self._z_hid_1 = tf.matmul(self._x, self._W_hid_1) + self._b_hid_1\n",
    "                self._h_hid_1 = tf.nn.relu(self._z_hid_1)\n",
    "\n",
    "                tf.summary.histogram('W_hid_1', self._W_hid_1)\n",
    "                tf.summary.histogram('b_hid_1', self._b_hid_1)\n",
    "                tf.summary.histogram('z_hid_1', self._z_hid_1)\n",
    "                \n",
    "                # Norm Ratio\n",
    "                self._W_hid_1_bu = tf.get_variable('hid_1_W_bu', trainable=False,\n",
    "                                                   initializer=self._W_hid_1.initialized_value())\n",
    "                norm_ratio = tf.norm( self._W_hid_1-self._W_hid_1_bu ) / tf.norm(self._W_hid_1_bu)\n",
    "                tf.summary.scalar( 'Param_Update_Norm_Ratio', norm_ratio )\n",
    "\n",
    "            with tf.variable_scope('Hidden_2'):\n",
    "                self._W_hid_2 = tf.get_variable('hid_2_W', shape=[nb_hid_1, nb_hid_2], dtype=tf.float32)\n",
    "                self._b_hid_2 = tf.get_variable('hid_2_b', shape=[nb_hid_2], dtype=tf.float32,\n",
    "                                               initializer=tf.zeros_initializer())\n",
    "                self._z_hid_2 = tf.matmul(self._h_hid_1, self._W_hid_2) + self._b_hid_2\n",
    "                self._h_hid_2 = tf.nn.relu(self._z_hid_2)\n",
    "\n",
    "                tf.summary.histogram('W_hid_2', self._W_hid_2)\n",
    "                tf.summary.histogram('b_hid_2', self._b_hid_2)\n",
    "                tf.summary.histogram('z_hid_2', self._z_hid_2)\n",
    "                \n",
    "                # Norm Ratio\n",
    "                self._W_hid_2_bu = tf.get_variable('hid_2_W_bu', trainable=False,\n",
    "                                                   initializer=self._W_hid_2.initialized_value())\n",
    "                norm_ratio = tf.norm( self._W_hid_2-self._W_hid_2_bu ) / tf.norm(self._W_hid_2_bu)\n",
    "                tf.summary.scalar( 'Param_Update_Norm_Ratio', norm_ratio )\n",
    "\n",
    "            with tf.variable_scope('Output'):\n",
    "                self._W_out = tf.get_variable('out_W', shape=[nb_hid_2, nb_out], dtype=tf.float32)\n",
    "                self._b_out = tf.get_variable('out_b', shape=[nb_out],\n",
    "                                             initializer=tf.zeros_initializer())\n",
    "                self._y_hat = tf.matmul(self._h_hid_2, self._W_out) + self._b_out\n",
    "\n",
    "                tf.summary.histogram('W_out', self._W_out)\n",
    "                tf.summary.histogram('b_out', self._b_out)\n",
    "                tf.summary.histogram('y_hat', self._y_hat)\n",
    "                \n",
    "                # Norm Ratio\n",
    "                self._W_out_bu = tf.get_variable('out_W_bu', trainable=False,\n",
    "                                                   initializer=self._W_out.initialized_value())\n",
    "                norm_ratio = tf.norm( self._W_out-self._W_out_bu ) / tf.norm(self._W_out_bu)\n",
    "                tf.summary.scalar( 'Param_Update_Norm_Ratio', norm_ratio )\n",
    "\n",
    "\n",
    "            with tf.variable_scope('zz_Loss'):\n",
    "                self._loss = tf.losses.mean_squared_error(self._y, self._y_hat)\n",
    "                tf.summary.scalar('loss', self._loss)\n",
    "\n",
    "            self._optimizer = tf.train.RMSPropOptimizer(learning_rate=lr)\n",
    "            #self._optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.025)\n",
    "            grads_and_vars = self._optimizer.compute_gradients(self._loss)\n",
    "            \n",
    "            \n",
    "            with tf.control_dependencies([tf.assign(self._W_hid_1_bu, self._W_hid_1),\n",
    "                                          tf.assign(self._W_hid_2_bu, self._W_hid_2),\n",
    "                                          tf.assign(self._W_out_bu, self._W_out)]):\n",
    "                self._train_op = self._optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "            self._dW_hid_1 = grads_and_vars[0][0]\n",
    "            self._db_hid_1 = grads_and_vars[1][0]\n",
    "            self._dW_hid_2 = grads_and_vars[2][0]\n",
    "            self._db_hid_2 = grads_and_vars[3][0]\n",
    "            self._dW_out = grads_and_vars[4][0]\n",
    "            self._db_out = grads_and_vars[5][0]\n",
    "            \n",
    "            with tf.variable_scope('Hidden_1'):\n",
    "                tf.summary.scalar('Gradient_Norm', tf.norm(self._dW_hid_1))\n",
    "            with tf.variable_scope('Hidden_2'):\n",
    "                tf.summary.scalar('Gradient_Norm', tf.norm(self._dW_hid_2))\n",
    "            with tf.variable_scope('Output'):\n",
    "                tf.summary.scalar('Gradient_Norm', tf.norm(self._dW_out))\n",
    "\n",
    "        self._merged_summaries = tf.summary.merge_all(scope='NeuralNet')\n",
    "        \n",
    "                \n",
    "    def setup_logdb(self, filehandle):\n",
    "        nb_in, nb_hid_1, nb_hid_2, nb_out = self.nb_in, self.nb_hid_1, self.nb_hid_2, self.nb_out\n",
    "        self._f = filehandle\n",
    "        self._f.create_earray(self._f.root, 'loss', atom=tables.Float32Atom(), shape=[0] )\n",
    "        \n",
    "        self._f.create_earray(self._f.root, 'x', atom=tables.Float32Atom(), shape=[0, 100, nb_in])\n",
    "        self._f.create_earray(self._f.root, 'y', atom=tables.Float32Atom(), shape=[0, 100, nb_out])\n",
    "        \n",
    "        self._f.create_earray(self._f.root, 'hid_1_z', atom=tables.Float32Atom(), shape=[0, 100, nb_hid_1] )\n",
    "        self._f.create_earray(self._f.root, 'hid_2_z', atom=tables.Float32Atom(), shape=[0, 100, nb_hid_2] )\n",
    "        self._f.create_earray(self._f.root, 'out_z', atom=tables.Float32Atom(), shape=[0, 100, nb_out] )\n",
    "        \n",
    "        self._f.create_earray(self._f.root, 'hid_1_dW', atom=tables.Float32Atom(), shape=[0, nb_in, nb_hid_1] )\n",
    "        self._f.create_earray(self._f.root, 'hid_1_db', atom=tables.Float32Atom(), shape=[0, nb_hid_1] )\n",
    "        self._f.create_earray(self._f.root, 'hid_2_dW', atom=tables.Float32Atom(), shape=[0, nb_hid_1, nb_hid_2] )\n",
    "        self._f.create_earray(self._f.root, 'hid_2_db', atom=tables.Float32Atom(), shape=[0, nb_hid_2] )\n",
    "        self._f.create_earray(self._f.root, 'out_dW', atom=tables.Float32Atom(), shape=[0, nb_hid_2, nb_out] )\n",
    "        self._f.create_earray(self._f.root, 'out_db', atom=tables.Float32Atom(), shape=[0, nb_out] )\n",
    "        \n",
    "        self._f.create_earray(self._f.root, 'hid_1_W', atom=tables.Float32Atom(), shape=[0, nb_in, nb_hid_1] )\n",
    "        self._f.create_earray(self._f.root, 'hid_1_b', atom=tables.Float32Atom(), shape=[0, nb_hid_1] )\n",
    "        self._f.create_earray(self._f.root, 'hid_2_W', atom=tables.Float32Atom(), shape=[0, nb_hid_1, nb_hid_2] )\n",
    "        self._f.create_earray(self._f.root, 'hid_2_b', atom=tables.Float32Atom(), shape=[0, nb_hid_2] )\n",
    "        self._f.create_earray(self._f.root, 'out_W', atom=tables.Float32Atom(), shape=[0, nb_hid_2, nb_out] )\n",
    "        self._f.create_earray(self._f.root, 'out_b', atom=tables.Float32Atom(), shape=[0, nb_out] )\n",
    "\n",
    "    def backward(self, x, y):\n",
    "        assert x.ndim == y.ndim == 2\n",
    "        \n",
    "        _, merged_summaries, y_hat, z_hid_2, z_hid_1, loss, \\\n",
    "            hid_1_dW, hid_1_db, hid_2_dW, hid_2_db, out_dW, out_db = self._sess.run(\n",
    "                [self._train_op, self._merged_summaries, self._y_hat, self._z_hid_2, self._z_hid_1, self._loss,\n",
    "                self._dW_hid_1, self._db_hid_1, self._dW_hid_2, self._db_hid_2, self._dW_out, self._db_out],\n",
    "                feed_dict={self._x: x, self._y:y})\n",
    "        \n",
    "        if self._summary_writer is not None:\n",
    "            self._summary_writer.add_summary(merged_summaries, self._timestep)\n",
    "        self._timestep += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self._f is not None:\n",
    "            \n",
    "            hid_1_W, hid_1_b, hid_2_W, hid_2_b, out_W, out_b = self._sess.run(tf.trainable_variables())\n",
    "            \n",
    "            self._f.root.loss.append(np.array(loss, ndmin=1))\n",
    "            \n",
    "            self._f.root.x.append(np.expand_dims(x[:100], axis=0))\n",
    "            self._f.root.y.append(np.expand_dims(y[:100], axis=0))\n",
    "                        \n",
    "            self._f.root.hid_1_z.append(np.expand_dims(z_hid_1[:100], axis=0))\n",
    "            self._f.root.hid_2_z.append(np.expand_dims(z_hid_2[:100], axis=0))\n",
    "            self._f.root.out_z.append(np.expand_dims(y_hat[:100], axis=0))\n",
    "            \n",
    "            self._f.root.hid_1_dW.append(np.expand_dims(hid_1_dW, axis=0))\n",
    "            self._f.root.hid_1_db.append(np.expand_dims(hid_1_db, axis=0))\n",
    "            self._f.root.hid_2_dW.append(np.expand_dims(hid_2_dW, axis=0))\n",
    "            self._f.root.hid_2_db.append(np.expand_dims(hid_2_db, axis=0))\n",
    "            self._f.root.out_dW.append(np.expand_dims(out_dW, axis=0))\n",
    "            self._f.root.out_db.append(np.expand_dims(out_db, axis=0))\n",
    "            \n",
    "            self._f.root.hid_1_W.append(np.expand_dims(hid_1_W, axis=0))\n",
    "            self._f.root.hid_1_b.append(np.expand_dims(hid_1_b, axis=0))\n",
    "            self._f.root.hid_2_W.append(np.expand_dims(hid_2_W, axis=0))\n",
    "            self._f.root.hid_2_b.append(np.expand_dims(hid_2_b, axis=0))\n",
    "            self._f.root.out_W.append(np.expand_dims(out_W, axis=0))\n",
    "            self._f.root.out_b.append(np.expand_dims(out_b, axis=0))\n",
    "        \n",
    "        return y_hat, loss\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._sess.run(self._y_hat, feed_dict={self._x: x})\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.saver(self._sess, filepath)\n",
    "        \n",
    "    def load(self, filepath):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self._sess, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFFunctApprox():\n",
    "\n",
    "    def __init__(self, model, st_low, st_high, rew_mean, rew_std, nb_actions):\n",
    "        \"\"\"Q-function approximator using Keras model\n",
    "\n",
    "        Args:\n",
    "            model: Keras compiled model\n",
    "        \"\"\"\n",
    "        st_low = np.array(st_low);    st_high = np.array(st_high)\n",
    "        self._model = model\n",
    "        \n",
    "        assert st_low.ndim == 1 and st_low.shape == st_high.shape\n",
    "        \n",
    "        if len(st_low) != model.nb_in:\n",
    "            raise ValueError('Input shape does not match state_space shape')\n",
    "\n",
    "        if nb_actions != model.nb_out:\n",
    "            raise ValueError('Output shape does not match action_space shape')\n",
    "\n",
    "        # normalise inputs\n",
    "        self._offsets = st_low + (st_high - st_low) / 2\n",
    "        self._scales = 1 / ((st_high - st_low) / 2)\n",
    "        \n",
    "        self._rew_mean = rew_mean\n",
    "        self._rew_std = rew_std\n",
    "\n",
    "    def eval(self, states):\n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert states.ndim == 2\n",
    "\n",
    "        inputs = (states - self._offsets) * self._scales\n",
    "\n",
    "        y_hat = self._model.forward(inputs)\n",
    "        \n",
    "        #return y_hat\n",
    "        return y_hat*self._rew_std + self._rew_mean\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        \n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert isinstance(actions, np.ndarray)\n",
    "        assert isinstance(targets, np.ndarray)\n",
    "        assert states.ndim == 2\n",
    "        assert actions.ndim == 1\n",
    "        assert targets.ndim == 1\n",
    "        assert len(states) == len(actions) == len(targets)\n",
    "        \n",
    "        targets = (targets-self._rew_mean) / self._rew_std    # decreases range (std>1) to approx -1..1\n",
    "\n",
    "        inputs = (states - self._offsets) * self._scales\n",
    "        all_targets = self._model.forward(inputs)             # this range should be small already\n",
    "        all_targets[np.arange(len(all_targets)), actions] = targets\n",
    "        self._model.backward(inputs, all_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"Circular buffer for DQN memory reply. Fairly fast.\"\"\"\n",
    "\n",
    "    def __init__(self, max_len, state_shape, state_dtype):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_len: maximum capacity\n",
    "        \"\"\"\n",
    "        assert isinstance(max_len, int)\n",
    "        assert max_len > 0\n",
    "\n",
    "        self.max_len = max_len                            # maximum length        \n",
    "        self._curr_insert_ptr = 0                          # index to insert next data sample\n",
    "        self._curr_len = 0                                 # number of currently stored elements\n",
    "\n",
    "        state_arr_shape = [max_len] + list(state_shape)\n",
    "\n",
    "        self._hist_St = np.zeros(state_arr_shape, dtype=state_dtype)\n",
    "        self._hist_At = np.zeros(max_len, dtype=int)\n",
    "        self._hist_Rt_1 = np.zeros(max_len, dtype=float)\n",
    "        self._hist_St_1 = np.zeros(state_arr_shape, dtype=state_dtype)\n",
    "        self._hist_done_1 = np.zeros(max_len, dtype=bool)\n",
    "\n",
    "    def append(self, St, At, Rt_1, St_1, done_1):\n",
    "        \"\"\"Add one sample to memory, override oldest if max_len reached.\n",
    "\n",
    "        Args:\n",
    "            St [np.ndarray]   - state\n",
    "            At [int]          - action\n",
    "            Rt_1 [float]      - reward\n",
    "            St_1 [np.ndarray] - next state\n",
    "            done_1 [bool]       - next state terminal?\n",
    "        \"\"\"\n",
    "        self._hist_St[self._curr_insert_ptr] = St\n",
    "        self._hist_At[self._curr_insert_ptr] = At\n",
    "        self._hist_Rt_1[self._curr_insert_ptr] = Rt_1\n",
    "        self._hist_St_1[self._curr_insert_ptr] = St_1\n",
    "        self._hist_done_1[self._curr_insert_ptr] = done_1\n",
    "        \n",
    "        if self._curr_len < self.max_len:                 # keep track of current length\n",
    "            self._curr_len += 1\n",
    "            \n",
    "        self._curr_insert_ptr += 1                         # increment insertion pointer\n",
    "        if self._curr_insert_ptr >= self.max_len:         # roll to zero if needed\n",
    "            self._curr_insert_ptr = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of samples in memory, 0 <= length <= max_len\"\"\"\n",
    "        return self._curr_len\n",
    "\n",
    "    def get_batch(self, batch_len):\n",
    "        \"\"\"Sample batch of data, with repetition\n",
    "\n",
    "        Args:\n",
    "            batch_len: nb of samples to pick\n",
    "\n",
    "        Returns:\n",
    "            states, actions, rewards, next_states, next_done, indices\n",
    "            Each returned element is np.ndarray with length == batch_len\n",
    "        \"\"\"\n",
    "        assert self._curr_len > 0\n",
    "        assert batch_len > 0\n",
    "\n",
    "        \n",
    "        indices = np.random.randint(                   # randint much faster than np.random.sample\n",
    "            low=0, high=self._curr_len, size=batch_len, dtype=int)\n",
    "\n",
    "        states = np.take(self._hist_St, indices, axis=0)\n",
    "        actions = np.take(self._hist_At, indices, axis=0)\n",
    "        rewards_1 = np.take(self._hist_Rt_1, indices, axis=0)\n",
    "        states_1 = np.take(self._hist_St_1, indices, axis=0)\n",
    "        dones_1 = np.take(self._hist_done_1, indices, axis=0)\n",
    "\n",
    "        return states, actions, rewards_1, states_1, dones_1, indices\n",
    "\n",
    "\n",
    "    \n",
    "    def pick_last(self, nb):\n",
    "        \"\"\"Pick last nb elements from memory\n",
    "        \n",
    "        Returns:\n",
    "            states, actions, rewards, next_states, done_1, indices\n",
    "            Each returned element is np.ndarray with length == batch_len\n",
    "        \"\"\"\n",
    "        assert nb <= self._curr_len\n",
    "        \n",
    "        start = self._curr_insert_ptr - nb                # inclusive\n",
    "        end = self._curr_insert_ptr                       # not inclusive\n",
    "        indices = np.array(range(start,end), dtype=int)   # indices to pick, can be negative\n",
    "        indices[indices < 0] += self._curr_len            # loop negative to positive\n",
    "        \n",
    "        states = np.take(self._hist_St, indices, axis=0)\n",
    "        actions = np.take(self._hist_At, indices, axis=0)\n",
    "        rewards_1 = np.take(self._hist_Rt_1, indices, axis=0)\n",
    "        states_1 = np.take(self._hist_St_1, indices, axis=0)\n",
    "        dones_1 = np.take(self._hist_done_1, indices, axis=0)\n",
    "        \n",
    "        return states, actions, rewards_1, states_1, dones_1, indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

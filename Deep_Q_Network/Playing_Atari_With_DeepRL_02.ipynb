{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "$$ \\huge{\\underline{\\textbf{ Playing Atari Games with Deep RL }}} $$\n",
    "\n",
    "$$ \\large{\\textbf{MountainCar + DQN + Memory Reply}} $$\n",
    "\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(start_step, env, frames, gamma, eps_decay_steps, eps_target,\n",
    "               batch_size, model, mem, callback=None, trace=None):\n",
    "    \"\"\"Episodic Semi-Gradient Sarsa\n",
    "    \n",
    "    Params:\n",
    "        env - environment\n",
    "        ep - number of episodes to run\n",
    "        gamma - discount factor [0..1]\n",
    "        eps - epsilon-greedy param\n",
    "        model      - function approximator, already initialised, with methods:\n",
    "                     eval(state, action) -> float\n",
    "                     train(state, target) -> None\n",
    "    \"\"\"\n",
    "    def policy(st, model, eps):\n",
    "        if np.random.rand() > eps:\n",
    "            stack = np.stack([st])  # convert lazyframe to nn input shape [1, 84, 84, 4]\n",
    "            q_values = model.eval(stack)\n",
    "            return np.argmax(q_values)\n",
    "        else:\n",
    "            return env.action_space.sample()\n",
    "    \n",
    "    if eps_decay_steps is not None:\n",
    "        eps_delta = (1-eps_target) / eps_decay_steps\n",
    "        eps = 1 - start_step*eps_delta\n",
    "        eps = max(eps, eps_target)\n",
    "    else:\n",
    "        eps = eps_target\n",
    "        \n",
    "    assert len(mem) >= batch_size\n",
    "    \n",
    "    tts_ = 0                                 # total time step\n",
    "    for e_ in itertools.count():             # count from 0 to infinity\n",
    "        \n",
    "        S = env.reset()\n",
    "        env.render()\n",
    "        \n",
    "        for t_ in itertools.count():         # count from 0 to infinity\n",
    "            \n",
    "            A = policy(S, model, eps)\n",
    "            \n",
    "            S_, R, done, _ = env.step(A)\n",
    "            env.render()\n",
    "            \n",
    "            mem.append(S, A, R, S_, done)\n",
    "            \n",
    "            if callback is not None:\n",
    "                callback(tts_+start_step, e_, t_, S, A, R, done, eps, model, mem, trace)\n",
    "            \n",
    "            states, actions, rewards, n_states, dones, _ = mem.get_batch(batch_size)\n",
    "            targets = model.eval(n_states)\n",
    "            targets = rewards + gamma * np.max(targets, axis=-1)\n",
    "            targets[dones] = rewards[dones]                # return of next-to-terminal state is just R\n",
    "            model.train(states, actions, targets)\n",
    "\n",
    "            tts_ += 1\n",
    "            if tts_ >= frames:\n",
    "                return\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            S = S_\n",
    "            \n",
    "            if eps > eps_target:\n",
    "                eps = max(eps - eps_delta, eps_target)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, frames, episodes, eps, model, callback=None, trace=None, render=True, sleep=0):\n",
    "\n",
    "    def policy(st, model, eps):\n",
    "        if np.random.rand() > eps:\n",
    "            stack = np.stack([st])  # convert lazyframe to nn input shape [1, 84, 84, 4]\n",
    "            q_values = model.eval(stack)\n",
    "            return np.argmax(q_values)\n",
    "        else:\n",
    "            return env.action_space.sample()\n",
    "        \n",
    "    total_reward = 0\n",
    "    \n",
    "    tts_ = 0                                 # total time step\n",
    "    for e_ in itertools.count():             # count from 0 to infinity\n",
    "        \n",
    "        S = env.reset()\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(sleep)\n",
    "        \n",
    "        for t_ in itertools.count():         # count from 0 to infinity\n",
    "            \n",
    "            A = policy(S, model, eps)\n",
    "            \n",
    "            S_, R, done, _ = env.step(A)\n",
    "            \n",
    "            total_reward += R\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(sleep)\n",
    "            \n",
    "            if callback is not None:\n",
    "                callback(tts_, e_, t_, S, A, R, done, eps, model, None, trace)\n",
    "    \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if frames is not None and tts_ >= frames:\n",
    "                return total_reward\n",
    "                \n",
    "            S = S_\n",
    "                \n",
    "            tts_ += 1\n",
    "            \n",
    "        if episodes is not None and e_ >= episodes-1:\n",
    "            return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_fill(env, mem, steps=None, episodes=None, render=False):\n",
    "        \n",
    "    # Fill memory buffer using random policy\n",
    "    tts_ = 0\n",
    "    for e_ in itertools.count():\n",
    "        if episodes is not None and e_ >= episodes:\n",
    "            return\n",
    "        \n",
    "        S = env.reset();\n",
    "        if render: env.render()\n",
    "        \n",
    "        for t_ in itertools.count():\n",
    "        \n",
    "            A = env.action_space.sample()    # random policy\n",
    "            S_, R, done, _ = env.step(A)\n",
    "            if render: env.render()\n",
    "                \n",
    "            mem.append(S, A, R, S_, done)\n",
    "            \n",
    "            tts_ += 1\n",
    "            \n",
    "            if steps is not None and tts_ >= steps:\n",
    "                return\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            S = S_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports (source file: [tiles3.py](tiles3.py), [helpers_1001.py](helpers_1001.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tables\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import PIL\n",
    "import gym\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "with tf.Session(config=config) as sess:\n",
    "    devs = sess.list_devices()\n",
    "    print('\\n'.join([x.name for x in devs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "import importlib\n",
    "importlib.reload(helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Debug_NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tables_logger\n",
    "importlib.reload(tables_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need callback to capture q-value array for whole state-action space at specified episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trace():\n",
    "    def __init__(self):\n",
    "            \n",
    "#         self.eval_every = eval_every\n",
    "#         self.render = render\n",
    "#         self.test_states = test_states\n",
    "#         self.test_labels = test_labels\n",
    "#         self.state_labels = state_labels\n",
    "        \n",
    "        self.total_step = 0\n",
    "        \n",
    "        self.epsilons = []\n",
    "        \n",
    "#         self.q_values = collections.OrderedDict()\n",
    "        self.ep_end_idx = collections.OrderedDict()\n",
    "        self.ep_rewards = collections.defaultdict(float)\n",
    "        \n",
    "#         self.states = []\n",
    "#         self.actions = []\n",
    "#         self.rewards = []  # t+1\n",
    "#         self.dones = []    # t+1\n",
    "#         self.epsilons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(total_step, episode, tstep, st, act, rew_, done_, eps, model, memory, trace):\n",
    "    \"\"\"Called from gradient_MC after every episode.\n",
    "    \n",
    "    Params:\n",
    "        episode [int] - episode number\n",
    "        tstep [int]   - timestep within episode\n",
    "        model [obj]   - function approximator\n",
    "        trace [list]  - list to write results to\"\"\"\n",
    "        \n",
    "    if done_:\n",
    "        trace.ep_end_idx[episode] = trace.total_step\n",
    "    \n",
    "#     trace.states.append(st)\n",
    "#     trace.actions.append(act)\n",
    "#     trace.rewards.append(rew_)\n",
    "#     trace.dones.append(done_)\n",
    "#     trace.epsilons.append(eps)\n",
    "    \n",
    "    trace.ep_rewards[episode] += rew_\n",
    "    \n",
    "    trace.epsilons.append(eps)\n",
    "    \n",
    "#     if trace.render:\n",
    "#         env.render()\n",
    "            \n",
    "#     if trace.eval_every is not None:\n",
    "#         if trace.total_step % trace.eval_every == 0:\n",
    "#             print()\n",
    "#             print('â– '*80)\n",
    "#             print('episode:', episode, '\\t time step:', tstep,\n",
    "#                   '\\t total time step:', trace.total_step, '\\t eps:', round(eps,3), \n",
    "#                   '\\t wall time:', datetime.datetime.now())\n",
    "\n",
    "            \n",
    "#         if trace.total_step % trace.eval_every == 0:\n",
    "            \n",
    "#             if len(st) == 2:\n",
    "#                 # We are working with 2D environment,\n",
    "#                 # plot whole Q-Value functions across whole state space\n",
    "            \n",
    "#                 q_arr = helpers.eval_state_action_space(model, env, split=[128,128])\n",
    "#                 trace.q_values[trace.total_step] = q_arr\n",
    "\n",
    "#                 helpers.plot_mountain_car(env, episode, trace.total_step, 1000, trace, memory,\n",
    "#                                           axis_labels=['state[0]', 'state[1]'],\n",
    "#                                           action_labels=['Act 0', 'Act 1', 'Act 2'],\n",
    "#                                           action_colors=['red', 'blue', 'green'])\n",
    "                \n",
    "#             else:\n",
    "#                 # Environment is not 2D, so we can't plot whole Q-Value function\n",
    "#                 # Instead we plot state on standard graph, which is still better than nothing\n",
    "                \n",
    "#                 if trace.test_states is not None:\n",
    "#                     y_hat = model.eval(trace.test_states)\n",
    "#                     trace.q_values[trace.total_step] = y_hat\n",
    "                \n",
    "#                 helpers.plot_generic_environment(env, trace.total_step, 1000, trace, memory)\n",
    "\n",
    "    assert total_step == trace.total_step            \n",
    "    \n",
    "    trace.total_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pong Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env in gym.envs.registry.all():\n",
    "    if env.id.startswith('Pong'):\n",
    "        print(env.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(obs):\n",
    "    obs_rgb = rgb2gray(obs)\n",
    "    obs_110x84 = resize(obs_rgb, output_shape=(110, 84), mode='reflect', anti_aliasing=True)\n",
    "    obs_84x84 = obs_110x84[13:-13,:]\n",
    "    obs_uint8 = (obs_84x84*255).astype(np.uint8)\n",
    "    return obs_uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(obs):\n",
    "    img = PIL.Image.fromarray(obs)\n",
    "    img = img.convert('L')\n",
    "    img = img.resize([84, 84], resample=PIL.Image.BILINEAR, box=[0,34,160,160+34])\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(obs):\n",
    "    img = PIL.Image.fromarray(obs)\n",
    "    img = img.convert('L')\n",
    "    img = img.resize([84, 84], resample=PIL.Image.NEAREST, box=[0,34,160,160+34])\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frames(frames):\n",
    "    stack = np.array(frames)  # convert LazyFrame to np.ndarray\n",
    "    assert stack.shape == (84, 84, 4)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=stack.shape[-1], figsize=[16,4])\n",
    "    for i in range(stack.shape[-1]):\n",
    "        axes[i].imshow(stack[:,:,i], cmap='gray', vmin=0, vmax=255)\n",
    "        axes[i].set_title('frame '+str(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyFrames:\n",
    "    def __init__(self, frames):\n",
    "        assert isinstance(frames, list)\n",
    "        assert isinstance(frames[0], np.ndarray)\n",
    "        self._frames = frames   # list of np.ndarray\n",
    "        \n",
    "    def __array__(self, dtype=None):\n",
    "        # print('__ARRAY__ called')\n",
    "        merged = np.stack(self._frames, axis=-1)\n",
    "        if dtype is not None:\n",
    "            merged = merged.astype(dtype)\n",
    "        return merged\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(np.round(np.stack(self._frames, axis=-1), decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapAtari:\n",
    "    def __init__(self, env):\n",
    "        assert env.observation_space == gym.spaces.Box(low=0, high=255, shape=[210,160,3], dtype=np.uint8)\n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=[84, 84, 4], dtype=np.uint8)\n",
    "        self.action_space = env.action_space\n",
    "        \n",
    "        self._env = env\n",
    "        self._frames = collections.deque(maxlen=4)\n",
    "    \n",
    "    def reset(self):\n",
    "        raw_obs = self._env.reset()           # 160x120 RGB\n",
    "        obs = preprocess(raw_obs)             # 84x84 grayscale\n",
    "        for _ in range(self._frames.maxlen):\n",
    "            self._frames.append(obs)          # replace all\n",
    "        return LazyFrames(list(self._frames))\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        raw_obs, rew, done, info = self._env.step(action)\n",
    "        obs = preprocess(raw_obs)             # 84x84 grayscale\n",
    "        self._frames.append(obs)\n",
    "        return LazyFrames(list(self._frames)), np.sign(rew), done, info\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self._env.render(mode=mode)\n",
    "    \n",
    "    def close(self):\n",
    "        self._env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movning Dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moving_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(moving_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('MovingDot-v0')\n",
    "env.max_steps = 100\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = TFNeuralNet(nb_out=5, logdir='tf_log_2/movingdot/test')\n",
    "# cnn.setup_logdb('outarray.h5', 5)\n",
    "model = TFFunctApprox(cnn, st_low=0, st_high=255, rew_mean=0, rew_std=1, nb_actions=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mem = Memory(max_len=10000, state_shape=(), state_dtype=object)\n",
    "mem_fill(env, mem, steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = Trace()\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning(trace.total_step, env, frames=10000, gamma=.95, eps_decay_steps=50000, eps_target=0.1,\n",
    "           batch_size=32, model=model, mem=mem, callback=callback, trace=trace)\n",
    "tr = evaluate(env, None, episodes=3, eps=0.05, model=model, render=True)\n",
    "print('tr', tr)\n",
    "rewards.append(tr)\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while trace.total_step < 25000:\n",
    "    q_learning(trace.total_step, env, frames=5000, gamma=.95, eps_decay_steps=10000, eps_target=0.1,\n",
    "           batch_size=32, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    # tr = evaluate(env, 10000, None, eps=0.05, model=model, render=True)\n",
    "    tr = evaluate(env, None, episodes=10, eps=0.05, model=model, render=True)\n",
    "    # cnn.save('./tf_models/PongDeterministic-v4_'+ str(trace.total_step) + '.ckpt')\n",
    "    print('iter', trace.total_step, 'tr', tr)\n",
    "    rewards.append(tr)\n",
    "    plt.plot(rewards)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trace.epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trace.ep_rewards.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pong - test test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work, 300k interation, Mem reply 50k, epsilon 1.0->0.1 over 50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = TFNeuralNet(nb_out=6, logdir='tf_log_2/pong/5')\n",
    "#cnn.setup_logdb('outarray.h5', 5)\n",
    "model = TFFunctApprox(cnn, st_low=0, st_high=255, rew_mean=0, rew_std=1, nb_actions=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(max_len=200000, state_shape=(), state_dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f preprocess mem_fill(env, mem, steps=10000)\n",
    "mem_fill(env, mem, steps=10000, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = Trace()\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr = evaluate(env, 10000, None, eps=0.05, model=model, render=True)\n",
    "tr = evaluate(env, None, episodes=3, eps=0.0, model=model, render=True)\n",
    "print('tr', tr)\n",
    "rewards.append(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while trace.total_step < 300000:\n",
    "    q_learning(trace.total_step, env, frames=50000, gamma=.95, eps_decay_steps=50000, eps_target=0.1,\n",
    "           batch_size=32, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    # tr = evaluate(env, 10000, None, eps=0.05, model=model, render=True)\n",
    "    tr = evaluate(env, None, episodes=3, eps=0.0, model=model, render=True)\n",
    "    # cnn.save('./tf_models/PongDeterministic-v4_'+ str(trace.total_step) + '.ckpt')\n",
    "    print('iter', trace.total_step, 'tr', tr)\n",
    "    rewards.append(tr)\n",
    "    plt.plot(rewards)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = evaluate(env, 10000, None, eps=0.0, model=model, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout - test test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = TFNeuralNet(nb_out=env.action_space.n)\n",
    "#cnn.setup_logdb('outarray.h5', 5)\n",
    "model = TFFunctApprox(cnn, st_low=0, st_high=255, rew_mean=0, rew_std=1, nb_actions=env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(max_len=1000000, state_shape=(), state_dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_fill(env, mem, steps=50000, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = Trace()\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = evaluate(env, 10000, None, eps=0.05, model=model, render=True)\n",
    "cnn.save('./tf_models/BreakoutDeterministic-v4_'+ str(trace.total_step) + '.ckpt')\n",
    "print('iter', trace.total_step, 'tr', tr)\n",
    "rewards.append(tr)\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while trace.total_step < 10000000:\n",
    "    q_learning(trace.total_step, env, frames=50000, gamma=.95, eps_decay_steps=1000000, eps_target=0.1,\n",
    "           batch_size=32, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    tr = evaluate(env, 10000, None, eps=0.05, model=model, render=True)\n",
    "    cnn.save('./tf_models/BreakoutDeterministic-v4_'+ str(trace.total_step) + '.ckpt')\n",
    "    print('iter', trace.total_step, 'tr', tr)\n",
    "    rewards.append(tr)\n",
    "    plt.plot(rewards)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_pong(frames):\n",
    "    \n",
    "    cnn = TFNeuralNet(nb_out=6)\n",
    "    #cnn.setup_logdb('outarray.h5', 5)\n",
    "    \n",
    "    model = TFFunctApprox(cnn, st_low=0, st_high=255, rew_mean=0, rew_std=1, nb_actions=6)\n",
    "    \n",
    "    mem = Memory(max_len=200000, state_shape=(), state_dtype=object)\n",
    "    \n",
    "#     trace = Trace(eval_every=1000,\n",
    "#                  test_states=np.array([[0, 1.4, 0, 0, 0, 0, 0, 0],     # init\n",
    "#                                        [0, 0.7, 0, 0, 0, 0, 0, 0],     # half way, no tilt\n",
    "#                                        [0, 0.0, 0, 0, 0, 0, 0, 0],]),  # landing pad\n",
    "#                  test_labels=['start', 'half-way', 'landing-pad'],\n",
    "#                  state_labels=['Pos.x', 'Pos.y', 'Vel.x', 'Vel.y', 'Angle', 'Ang. Vel', 'Left Leg', 'Right Leg'])\n",
    "    \n",
    "    \n",
    "    mem_fill(env, mem, steps=10000)\n",
    "    \n",
    "    if frames != 0:\n",
    "        \n",
    "        q_learning(0, env, frames=frames, gamma=.95, eps_decay_steps=50000, eps_target=0.1,\n",
    "                   batch_size=32, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    return trace, model, mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, don't train\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)\n",
    "trace_rl, model, mem = experiment_pong(frames=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train from scratch\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "trace_rl, model, mem = experiment_lunarlander_tf(frames=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "model._model.save('./tf_models/PongNoFrameskip-v4.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights\n",
    "model._model.load('./tf_models/PongNoFrameskip-v4.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_disp(total_step, episode, tstep, st, act, rew_, done_, eps, model, memory, trace):\n",
    "    if done_:\n",
    "        print(rew_, done_)\n",
    "    pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Enjoy agent\n",
    "try:\n",
    "    enjoy_env(env, frames=float('inf'), eps=1.0, model=model, callback=callback_disp)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enjoy Random Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS WORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, don't train\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_disp(total_step, episode, tstep, st, act, rew_, done_, eps, model, memory, trace):\n",
    "    global axes\n",
    "    if rew_ != 0:\n",
    "        print('rew:', rew_)\n",
    "    if done_:\n",
    "        print('done:', done_)\n",
    "    # plot_frames(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Enjoy agent\n",
    "try:\n",
    "    enjoy_env(env, frames=float('inf'), episodes=1, eps=1.0, model=None, callback=callback_disp)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Approximators and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFNeuralNet():\n",
    "    def __init__(self, nb_out, logdir):\n",
    "        \n",
    "        self.nb_out = nb_out\n",
    "        self._time_step = 0\n",
    "        \n",
    "        try:    sess.close()\n",
    "        except: pass\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self._log_filename = None\n",
    "        self._dict_layers = {}\n",
    "\n",
    "        self._x = tf.placeholder(name='x', shape=[None, 84, 84, 4], dtype=tf.float32)\n",
    "        self._y = tf.placeholder(name='y', shape=[None, nb_out], dtype=tf.float32)\n",
    "\n",
    "        model = tf.layers.conv2d(self._x, filters=16, kernel_size=[8, 8], strides=[4, 4],\n",
    "                                 padding='valid', activation=tf.nn.relu, name='Conv_1')\n",
    "            \n",
    "        model = tf.layers.conv2d(model, filters=32, kernel_size=[4, 4], strides=[2, 2],\n",
    "                                 padding='valid', activation=tf.nn.relu, name='Conv_2')\n",
    "        \n",
    "        model = tf.layers.flatten(model)\n",
    "        model = tf.layers.dense(model, 256, activation=tf.nn.relu, name='Dense')\n",
    "        self._y_hat = tf.layers.dense(model, nb_out, activation=None, name='Output')\n",
    "        \n",
    "        # self._mse = tf.reduce_mean( tf.pow(self._y - self._y_hat, 2) )\n",
    "        self._loss = tf.losses.mean_squared_error(self._y, self._y_hat)\n",
    "\n",
    "        # No gradient clipping\n",
    "        self._optimizer = tf.train.AdamOptimizer(learning_rate=0.00025)\n",
    "        # self._optimizer = tf.train.RMSPropOptimizer(learning_rate=0.00025, decay=0.0, momentum=0.95, epsilon=0.01)\n",
    "        self._grads_and_vars = self._optimizer.compute_gradients(self._loss)\n",
    "        self._train_op = self._optimizer.apply_gradients(self._grads_and_vars)\n",
    "        \n",
    "        # Global gradient clipping\n",
    "#         self._optimizer = tf.train.RMSPropOptimizer(learning_rate=0.00025, decay=0.0, momentum=0.95, epsilon=0.01)\n",
    "#         gradients, variables = zip(*self._optimizer.compute_gradients(self._loss))\n",
    "#         gradients, _ = tf.clip_by_global_norm(gradients, 1)\n",
    "#         self._train_op = self._optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        # Per matrix\n",
    "#         self._optimizer = tf.train.RMSPropOptimizer(learning_rate=0.00025, decay=0.0, momentum=0.95, epsilon=0.01)\n",
    "#         gradients, variables = zip(*self._optimizer.compute_gradients(self._loss))\n",
    "#         gradients = [ None if gradient is None else tf.clip_by_norm(gradient, 1.0) for gradient in gradients ]\n",
    "#         self._train_op = self._optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        tf.summary.scalar('loss', self._loss)\n",
    "\n",
    "        self._sess = tf.Session()\n",
    "        self._sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self._merged_summaries = tf.summary.merge_all()\n",
    "        self._writer = tf.summary.FileWriter(logdir=logdir, graph=self._sess.graph)\n",
    "        self._writer.flush()\n",
    "                \n",
    "    def backward(self, x, y):\n",
    "        assert x.ndim == 4\n",
    "        assert y.ndim == 2\n",
    "        assert x.shape == (32, 84, 84, 4)\n",
    "        \n",
    "        dict_layers, merged_summaries, _, loss = \\\n",
    "            self._sess.run([self._dict_layers, self._merged_summaries, self._train_op, self._loss],\n",
    "                            feed_dict={self._x: x, self._y: y})\n",
    "        \n",
    "        self._writer.add_summary(merged_summaries, self._time_step)\n",
    "        self._time_step += 1\n",
    "        \n",
    "        if self._log_filename is not None:\n",
    "            tables_logger.append_log(self._log_filename, dict_layers)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._sess.run(self._y_hat, feed_dict={self._x: x})\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self._sess, filepath)\n",
    "        \n",
    "    def load(self, filepath):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self._sess, filepath)\n",
    "        \n",
    "    def setup_logdb(self, filename, batch_save):\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "\n",
    "        dict_inout = {\n",
    "            #'batch_x' : cnn._x[0:batch_save,:,:,:],\n",
    "            'batch_y' : cnn._y[0:batch_save,:],\n",
    "        }\n",
    "\n",
    "        dict_conv_1 = {\n",
    "            'W': graph.get_tensor_by_name('Conv_1/kernel:0'),\n",
    "            'b': graph.get_tensor_by_name('Conv_1/bias:0'),\n",
    "            'dW': graph.get_tensor_by_name('gradients/Conv_1/Conv2D_grad/tuple/control_dependency_1:0'),\n",
    "            'db': graph.get_tensor_by_name('gradients/Conv_1/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "            'z': graph.get_tensor_by_name('Conv_1/BiasAdd:0')[0:batch_save,:,:,:],\n",
    "        }\n",
    "\n",
    "        dict_conv_2 = {\n",
    "            'W': graph.get_tensor_by_name('Conv_2/kernel:0'),\n",
    "            'b': graph.get_tensor_by_name('Conv_2/bias:0'),\n",
    "            'dW': graph.get_tensor_by_name('gradients/Conv_2/Conv2D_grad/tuple/control_dependency_1:0'),\n",
    "            'db': graph.get_tensor_by_name('gradients/Conv_2/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "            'z': graph.get_tensor_by_name('Conv_2/BiasAdd:0')[0:batch_save,:,:,:],\n",
    "        }\n",
    "\n",
    "        dict_dense = {\n",
    "            'W': graph.get_tensor_by_name('Dense/kernel:0')[:100,:50],\n",
    "            'b': graph.get_tensor_by_name('Dense/bias:0'),\n",
    "            'dW': graph.get_tensor_by_name('gradients/Dense/MatMul_grad/tuple/control_dependency_1:0')[:100,:50],\n",
    "            'db': graph.get_tensor_by_name('gradients/Dense/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "            'z': graph.get_tensor_by_name('Dense/BiasAdd:0')[0:batch_save,:],\n",
    "        }\n",
    "\n",
    "        dict_output = {\n",
    "            'W': graph.get_tensor_by_name('Output/kernel:0'),\n",
    "            'b': graph.get_tensor_by_name('Output/bias:0'),\n",
    "            'dW': graph.get_tensor_by_name('gradients/Output/MatMul_grad/tuple/control_dependency_1:0'),\n",
    "            'db': graph.get_tensor_by_name('gradients/Output/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "            'z': graph.get_tensor_by_name('Output/BiasAdd:0')[0:batch_save,:],\n",
    "        }\n",
    "\n",
    "        dict_metrics = {\n",
    "            'loss': cnn._loss,\n",
    "        }\n",
    "\n",
    "        self._log_filename = filename\n",
    "        self._dict_layers = {\n",
    "            'inout': dict_inout,\n",
    "            'conv_1': dict_conv_1,\n",
    "            'conv_2': dict_conv_2,\n",
    "            'dense': dict_dense,\n",
    "            'output': dict_output,\n",
    "            'metrics': dict_metrics,\n",
    "        }\n",
    "\n",
    "        tables_logger.create_log(filename, self._dict_layers, batch_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFFunctApprox():\n",
    "\n",
    "    def __init__(self, model, st_low, st_high, rew_mean, rew_std, nb_actions):\n",
    "        \"\"\"Q-function approximator using Keras model\n",
    "\n",
    "        Args:\n",
    "            model: Keras compiled model\n",
    "        \"\"\"\n",
    "        self._model = model\n",
    "        \n",
    "        assert np.isscalar(st_low) and np.isscalar(st_high)\n",
    "        \n",
    "        if nb_actions != model.nb_out:\n",
    "            raise ValueError('Output shape does not match action_space shape')\n",
    "\n",
    "        # normalise inputs\n",
    "        self._offsets = st_low + (st_high - st_low) / 2\n",
    "        self._scales = 1 / ((st_high - st_low) / 2)\n",
    "        \n",
    "        self._rew_mean = rew_mean\n",
    "        self._rew_std = rew_std\n",
    "\n",
    "    def eval(self, states):\n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert states.ndim == 4\n",
    "        assert states.shape == (32, 84, 84, 4) or states.shape == (1, 84, 84, 4)\n",
    "        \n",
    "        inputs = (states - self._offsets) * self._scales\n",
    "\n",
    "        y_hat = self._model.forward(inputs)\n",
    "        \n",
    "        return y_hat*self._rew_std + self._rew_mean\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        \n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert isinstance(actions, np.ndarray)\n",
    "        assert isinstance(targets, np.ndarray)\n",
    "        assert states.ndim == 4\n",
    "        assert actions.ndim == 1\n",
    "        assert targets.ndim == 1\n",
    "        assert len(states) == len(actions) == len(targets)\n",
    "        \n",
    "        targets = (targets-self._rew_mean) / self._rew_std    # decreases range (std>1) to approx -1..1\n",
    "\n",
    "        inputs = (states - self._offsets) * self._scales\n",
    "        all_targets = self._model.forward(inputs)             # this range should be small already\n",
    "        all_targets[np.arange(len(all_targets)), actions] = targets\n",
    "        return self._model.backward(inputs, all_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"Circular buffer for DQN memory reply. Fairly fast.\"\"\"\n",
    "\n",
    "    def __init__(self, max_len, state_shape, state_dtype):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_len: maximum capacity\n",
    "        \"\"\"\n",
    "        assert isinstance(max_len, int)\n",
    "        assert max_len > 0\n",
    "\n",
    "        self.max_len = max_len                            # maximum length        \n",
    "        self._curr_insert_ptr = 0                          # index to insert next data sample\n",
    "        self._curr_len = 0                                 # number of currently stored elements\n",
    "\n",
    "        state_arr_shape = [max_len] + list(state_shape)\n",
    "\n",
    "        self._hist_St = np.zeros(state_arr_shape, dtype=state_dtype)\n",
    "        self._hist_At = np.zeros(max_len, dtype=int)\n",
    "        self._hist_Rt_1 = np.zeros(max_len, dtype=float)\n",
    "        self._hist_St_1 = np.zeros(state_arr_shape, dtype=state_dtype)\n",
    "        self._hist_done_1 = np.zeros(max_len, dtype=bool)\n",
    "\n",
    "    def append(self, St, At, Rt_1, St_1, done_1):\n",
    "        \"\"\"Add one sample to memory, override oldest if max_len reached.\n",
    "\n",
    "        Args:\n",
    "            St [np.ndarray]   - state\n",
    "            At [int]          - action\n",
    "            Rt_1 [float]      - reward\n",
    "            St_1 [np.ndarray] - next state\n",
    "            done_1 [bool]       - next state terminal?\n",
    "        \"\"\"\n",
    "        self._hist_St[self._curr_insert_ptr] = St\n",
    "        self._hist_At[self._curr_insert_ptr] = At\n",
    "        self._hist_Rt_1[self._curr_insert_ptr] = Rt_1\n",
    "        self._hist_St_1[self._curr_insert_ptr] = St_1\n",
    "        self._hist_done_1[self._curr_insert_ptr] = done_1\n",
    "        \n",
    "        if self._curr_len < self.max_len:                 # keep track of current length\n",
    "            self._curr_len += 1\n",
    "            \n",
    "        self._curr_insert_ptr += 1                         # increment insertion pointer\n",
    "        if self._curr_insert_ptr >= self.max_len:         # roll to zero if needed\n",
    "            self._curr_insert_ptr = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of samples in memory, 0 <= length <= max_len\"\"\"\n",
    "        return self._curr_len\n",
    "\n",
    "    def get_batch(self, batch_len):\n",
    "        \"\"\"Sample batch of data, with repetition\n",
    "\n",
    "        Args:\n",
    "            batch_len: nb of samples to pick\n",
    "\n",
    "        Returns:\n",
    "            states, actions, rewards, next_states, next_done, indices\n",
    "            Each returned element is np.ndarray with length == batch_len\n",
    "        \"\"\"\n",
    "        assert self._curr_len > 0\n",
    "        assert batch_len > 0\n",
    "\n",
    "        \n",
    "        indices = np.random.randint(                   # randint much faster than np.random.sample\n",
    "            low=0, high=self._curr_len, size=batch_len, dtype=int)\n",
    "\n",
    "        states = np.take(self._hist_St, indices, axis=0)\n",
    "        actions = np.take(self._hist_At, indices, axis=0)\n",
    "        rewards_1 = np.take(self._hist_Rt_1, indices, axis=0)\n",
    "        states_1 = np.take(self._hist_St_1, indices, axis=0)\n",
    "        dones_1 = np.take(self._hist_done_1, indices, axis=0)\n",
    "        \n",
    "        if states.dtype == object and isinstance(mem._hist_St[0], LazyFrames): \n",
    "            states = np.stack(states)       # convert to single np.ndarray shape [batch_size, 4, 84, 84]\n",
    "            states_1 = np.stack(states_1)   # where '4' is number of history frames presented to agent\n",
    "\n",
    "        return states, actions, rewards_1, states_1, dones_1, indices\n",
    "\n",
    "\n",
    "    \n",
    "    def pick_last(self, nb):\n",
    "        \"\"\"Pick last nb elements from memory\n",
    "        \n",
    "        Returns:\n",
    "            states, actions, rewards, next_states, done_1, indices\n",
    "            Each returned element is np.ndarray with length == batch_len\n",
    "        \"\"\"\n",
    "        assert nb <= self._curr_len\n",
    "        \n",
    "        start = self._curr_insert_ptr - nb                # inclusive\n",
    "        end = self._curr_insert_ptr                       # not inclusive\n",
    "        indices = np.array(range(start,end), dtype=int)   # indices to pick, can be negative\n",
    "        indices[indices < 0] += self._curr_len            # loop negative to positive\n",
    "        \n",
    "        states = np.take(self._hist_St, indices, axis=0)\n",
    "        actions = np.take(self._hist_At, indices, axis=0)\n",
    "        rewards_1 = np.take(self._hist_Rt_1, indices, axis=0)\n",
    "        states_1 = np.take(self._hist_St_1, indices, axis=0)\n",
    "        dones_1 = np.take(self._hist_done_1, indices, axis=0)\n",
    "        \n",
    "        if states.dtype == object and isinstance(mem._hist_St[0], LazyFrames): \n",
    "            states = np.stack(states)       # convert to single np.ndarray shape [batch_size, 4, 84, 84]\n",
    "            states_1 = np.stack(states_1)   # where '4' is number of history frames presented to agent\n",
    "        \n",
    "        return states, actions, rewards_1, states_1, dones_1, indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "below is just testing\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pong NN Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(max_len=10000, state_shape=(), state_dtype=object)\n",
    "mem_fill(env, mem, one_episode=False)\n",
    "print(len(mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards, n_states, dones, _ = mem.pick_last(len(mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(rewards==-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(rewards==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(rewards==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del states\n",
    "del n_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = TFNeuralNet(nb_out=6)\n",
    "#cnn.setup_logdb('outarray.h5', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFFunctApprox(cnn, st_low=0, st_high=255, rew_mean=0, rew_std=1, nb_actions=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS SHOULD CONVERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "for i in range(50000):\n",
    "    states, actions, rewards, n_states, dones, _ = mem.get_batch(batch_size)\n",
    "    targets = model.eval(n_states)\n",
    "    targets = rewards + gamma * np.max(targets, axis=-1)\n",
    "    targets[dones] = rewards[dones]                # return of next-to-terminal state is just R\n",
    "    loss = model.train(states, actions, targets)\n",
    "    \n",
    "    losses.append(loss)\n",
    "    if i % 25 == 0:\n",
    "        print(i, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test CNN Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(max_len=1000, state_shape=(), state_dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_fill(env, mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards_1, states_1, dones_1, indices = mem.get_batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('----')\n",
    "    print(rewards_1[i])\n",
    "    plot_frames(states[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn = TFNeuralNet(nb_out=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards_1, states_1, dones_1, indices = mem.get_batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_nn = states / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.forward(states_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(logdir='tf_log', graph=cnn._sess.graph)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test CNN Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = TFNeuralNet(nb_out=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'outarray.h5'\n",
    "cnn.setup_logdb(filename, batch_save=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_logger.print_log(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Lazy Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([1, 1, 1])\n",
    "B = np.array([2, 2, 2])\n",
    "C = np.array([3, 3, 3])\n",
    "\n",
    "lf1 = LazyFrame([A, B])\n",
    "lf2 = LazyFrame([B, C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = np.zeros(shape=[10], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem[0] = lf1\n",
    "mem[1] = lf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf1._frames[0][0] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf1._frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack(mem[[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(mem[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = TFNeuralNet(nb_out=6)\n",
    "model = TFFunctApprox(cnn, st_low=0, st_high=255, rew_mean=0, rew_std=1, nb_actions=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_disp(total_step, episode, tstep, st, act, rew_, done_, eps, model, memory, trace):\n",
    "    if done_:\n",
    "        print(total_step)\n",
    "    # pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "tr = evaluate(env, 1000, None, eps=0.05, model=model, callback=callback_disp, render=True)\n",
    "print(time.time() - ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(10000, (), object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_fill(env, mem, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards_1, states_1, dones_1, indices = mem.pick_last(len(mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.count_nonzero(dones_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rew  1', np.count_nonzero(rewards_1==1))\n",
    "print('rew  0', np.count_nonzero(rewards_1==0))\n",
    "print('rew -1', np.count_nonzero(rewards_1==-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Mem Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MovingDot3-v0')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lframes = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lframes_, rew_, done_, _ = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(10, (), object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem.append(lframes, 0, rew_, lframes_, done_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lframes = lframes_\n",
    "lframes_, rew_, done_, _ = env.step(0)\n",
    "mem.append(lframes, 0, rew_, lframes_, done_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mem._hist_St)\n",
    "print(mem._hist_At)\n",
    "print(mem._hist_Rt_1)\n",
    "print(mem._hist_St_1)\n",
    "print(mem._hist_done_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.take(mem._hist_St, np.array([0, 1]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack(arr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frames(np.stack(arr)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frames(np.stack(arr)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards_1, states_1, dones_1, indices = mem.get_batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states.shape)\n",
    "print(actions.shape)\n",
    "print(rewards_1.shape)\n",
    "print(states_1.shape)\n",
    "print(dones_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_render(env, frames, episodes, eps, model, callback=None, trace=None, render=True, sleep=0):\n",
    "\n",
    "    rendered_frames = []\n",
    "    \n",
    "    def policy(st, model, eps):\n",
    "        if np.random.rand() > eps:\n",
    "            stack = np.stack([st])  # convert lazyframe to nn input shape [1, 84, 84, 4]\n",
    "            q_values = model.eval(stack)\n",
    "            return np.argmax(q_values)\n",
    "        else:\n",
    "            return env.action_space.sample()\n",
    "        \n",
    "    total_reward = 0\n",
    "    \n",
    "    tts_ = 0                                 # total time step\n",
    "    for e_ in itertools.count():             # count from 0 to infinity\n",
    "        \n",
    "        S = env.reset()\n",
    "        \n",
    "        if render:\n",
    "            rendered_frames.append(env.render(mode='rgb_array'))\n",
    "            time.sleep(sleep)\n",
    "        \n",
    "        for t_ in itertools.count():         # count from 0 to infinity\n",
    "            \n",
    "            A = policy(S, model, eps)\n",
    "            \n",
    "            S_, R, done, _ = env.step(A)\n",
    "            \n",
    "            total_reward += R\n",
    "            \n",
    "            if render:\n",
    "                rendered_frames.append(env.render(mode='rgb_array'))\n",
    "                time.sleep(sleep)\n",
    "            \n",
    "            if callback is not None:\n",
    "                callback(tts_, e_, t_, S, A, R, done, eps, model, None, trace)\n",
    "    \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if frames is not None and tts_ >= frames:\n",
    "                return rendered_frames\n",
    "                \n",
    "            S = S_\n",
    "                \n",
    "            tts_ += 1\n",
    "            \n",
    "        if episodes is not None and e_ >= episodes-1:\n",
    "            return rendered_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_frames = frames_render(env, frames=None, episodes=1, eps=0.0, model=model, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "fig = plt.figure(figsize=(rendered_frames[0].shape[1] / 72.0, rendered_frames[0].shape[0] / 72.0), dpi = 72)\n",
    "ax = fig.add_subplot(111);\n",
    "\n",
    "patch = ax.imshow(rendered_frames[0])\n",
    "# plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(i):\n",
    "    patch.set_data(rendered_frames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.FuncAnimation(fig, animate, frames=len(rendered_frames), interval=20, repeat=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
